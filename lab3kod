import csv
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score
import numpy as np

from transformers import pipeline, TFAutoModelForSequenceClassification, AutoTokenizer
import tensorflow as tf

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import precision_score, recall_score, f1_score




# .csv dosyasını açma ve okuma
def okuma():
    cümleler = []
    etiketler = []
    with open('twitter_training_azVeri.csv', newline='', encoding='utf-8') as csvfile:
        csvreader = csv.reader(csvfile)
        for row in csvreader:
            if len(row) >= 4:
                cümleler.append(row[3])  # 4. sütun: cümle
                etiketler.append(row[2])  # 3. sütun: etiket (pozitif, negatif, nötr)
    return cümleler, etiketler


def svm_model():
    print("Support Vector Machine modelini seçtiniz.")
    cümleler, etiketler = okuma()
    model, vectorizer = svm_model_egit(cümleler, etiketler)
    while True:
        yeni_cümle = input("Yeni bir cümle girin (çıkmak için 'q' tuşlayın): ")
        if yeni_cümle.lower() == 'q':
            break
        tahmin = cümle_tahmin_et_SVM(model, vectorizer, yeni_cümle)
        print(f"Tahmin edilen etiket: {tahmin}")
def svm_model_egit(cümleler, etiketler):
    # TF-IDF vektörizasyonu
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(cümleler)

    # Verileri eğitim ve test kümelerine ayırma
    X_train, X_test, y_train, y_test = train_test_split(X, etiketler, test_size=0.2, random_state=42)

    # SVM modelini eğitme
    model = svm.SVC(kernel='linear')
    model.fit(X_train, y_train)

    # Test verileri ile tahmin yapma
    predictions = model.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    print(f"Model doğruluğu: {accuracy}")
    # Precision hesaplama
    precision = precision_score(y_test, predictions, average='weighted')
    print(f"Precision: {precision}")

    # Recall hesaplama
    recall = recall_score(y_test, predictions, average='weighted')
    print(f"Recall: {recall}")

    # F-measure hesaplama
    f_measure = f1_score(y_test, predictions, average='weighted')
    print(f"F-measure: {f_measure}")

    return model, vectorizer
def cümle_tahmin_et_SVM(model, vectorizer, yeni_cümle):
    X_yeni = vectorizer.transform([yeni_cümle])
    tahmin = model.predict(X_yeni)
    return tahmin[0]


def large_model_egit():
    print("Alex Model modelini seçtiniz.")

    # Verileri okuma
    cümleler, etiketler = okuma()

    # Verileri eğitim ve test kümelerine ayırma
    X_train, X_test, y_train, y_test = train_test_split(cümleler, etiketler, test_size=0.2, random_state=42)

    # Etiketleri sayısal değerlere dönüştürme
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)
    y_test = le.transform(y_test)

    # Tokenizer ve modeli yükleme
    tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
    model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=len(le.classes_))

    # Eğitim verilerini tokenize etme
    train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128)
    test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=128)

    # TensorFlow dataset oluşturma
    train_dataset = tf.data.Dataset.from_tensor_slices((
        dict(train_encodings),
        y_train
    )).shuffle(1000).batch(16)

    test_dataset = tf.data.Dataset.from_tensor_slices((
        dict(test_encodings),
        y_test
    )).batch(16)

    # Modeli eğitme
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])

    model.fit(train_dataset, epochs=3, validation_data=test_dataset)

    # Test verileri ile tahmin yapma
    test_loss, test_accuracy = model.evaluate(test_dataset)
    print(f"Test doğruluğu: {test_accuracy}")


    return model, tokenizer, le
def cümle_tahmin_et_Large(model, tokenizer, le, yeni_cümle):
    inputs = tokenizer(yeni_cümle, return_tensors="tf", truncation=True, padding=True, max_length=128)
    outputs = model(inputs)
    logits = outputs.logits
    predicted_class = tf.argmax(logits, axis=1).numpy()[0]
    etiketler = le.inverse_transform([predicted_class])
    return etiketler[0]
def large_language_model():
    print("Large Language Model modelini seçtiniz. Aslında doğal dil işleme için BERT gibi bir model kullanacağız.")
    model, tokenizer, le = large_model_egit()
    while True:
        yeni_cümle = input("Yeni bir cümle girin (çıkmak için 'q' tuşlayın): ")
        if yeni_cümle.lower() == 'q':
            break
        tahmin = cümle_tahmin_et_Large(model, tokenizer, le, yeni_cümle)
        print(f"Tahmin edilen etiket: {tahmin}")




def alexnet_model_egit():
    print("AlexNet sınıflandırma yapacağız.")

    # Verileri okuma
    cümleler, etiketler = okuma()

    # Etiketleri sayısal değerlere dönüştürme
    le = LabelEncoder()
    y = le.fit_transform(etiketler)

    # Tokenize etme ve padding
    tokenizer = Tokenizer(num_words=5000)
    tokenizer.fit_on_texts(cümleler)
    X = tokenizer.texts_to_sequences(cümleler)
    X = pad_sequences(X, maxlen=100)

    # Verileri eğitim ve test kümelerine ayırma
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Modeli oluşturma
    model = Sequential()
    model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))
    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
    model.add(GlobalMaxPooling1D())
    model.add(Dense(10, activation='relu'))
    model.add(Dense(3, activation='softmax'))

    # Modeli derleme
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # Modeli eğitme
    model.fit(X_train, y_train, epochs=5, batch_size=16, validation_data=(X_test, y_test))

    # Test verileri ile tahmin yapma
    loss, accuracy = model.evaluate(X_test, y_test)
    print(f"Test doğruluğu: {accuracy}")



    return model, tokenizer, le
def cümle_tahmin_et_alexnet(model, tokenizer, le, yeni_cümle):
    seq = tokenizer.texts_to_sequences([yeni_cümle])
    padded = pad_sequences(seq, maxlen=100)
    pred = model.predict(padded)
    etiketler = le.inverse_transform([np.argmax(pred)])
    return etiketler[0]
def alexnet_model():
    print("Alexnet modelini seçtiniz.")
    model, tokenizer, le = alexnet_model_egit()
    while True:
        yeni_cümle = input("Yeni bir cümle girin (çıkmak için 'q' tuşlayın): ")
        if yeni_cümle.lower() == 'q':
            break
        tahmin = cümle_tahmin_et_alexnet(model, tokenizer, le, yeni_cümle)
        print(f"Tahmin edilen etiket: {tahmin}")






def menu():
    print("Lütfen bir seçenek seçin:")
    print("1 - Support Vector Machine")
    print("2 - AlexNet")
    print("3 - Large Language Model")
    print("4 - Çıkış")

def main():
    menu()
    choice = input("Seçiminizi girin (1, 2, 3 veya 4): ")

    if choice == '1':
        svm_model()
    elif choice == '2':
        alexnet_model()
    elif choice == '3':
        large_language_model()
    elif choice == '4':
        print("Programdan çıkılıyor...")
        return
    else:
        print("Geçersiz seçim. Lütfen 1, 2, 3 veya 4 seçin.")
    main()  # Ana menüyü tekrar çağırarak kullanıcıya yeni seçim yapma şansı verelim.

if __name__ == "__main__":
    main()

